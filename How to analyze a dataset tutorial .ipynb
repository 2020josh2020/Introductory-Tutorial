{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All meterial aggregated from the following sources \n",
    "\n",
    "#   MIT introduction to python \n",
    "#   a-z machine learning \n",
    "#   columbia intro to machine learning \n",
    "#   andrew ng - machine learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Projects \n",
    "\n",
    "# 1. The question \n",
    "# 2. Data Quality \n",
    "# 3. Do the features make sense \n",
    "#    a. appropriate labels \n",
    "#    b. scaling \n",
    "#    c. decimals vs integers \n",
    "# 4. Data Cleaning \n",
    "# 5. Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to analyze a dataset \n",
    "# step-by-step\n",
    "# 2/18/2019\n",
    "\n",
    "# Cell 1 \n",
    "# Documentation about the project \n",
    "# import all libraries needed \n",
    "\n",
    "# cell 2 \n",
    "# load data \n",
    "# df = pd.read_csv('yourdatafile.csv')\n",
    "\n",
    "# cell 3 \n",
    "# investigate your data\n",
    "# df.head(5)\n",
    "# df.columns() \n",
    "# rename your columns if necessary \n",
    "# df.columns= [\"col1name\", \"col2name\", ...]\n",
    "\n",
    "# cell 4 \n",
    "# explore dataset \n",
    "# df.shape\n",
    "# len(df)\n",
    "# df['record_id'].nunique()\n",
    "# df.dtypes \n",
    "# looking for strings that should be numeric \n",
    "# convert non-numeric to numeric \n",
    "# df['column'] = df['column'].apply(pd.to_numeric)\n",
    "# find unique categorical values in a single column \n",
    "# df['myCategoricalColumn'].value_counts(dropna = False)\n",
    "\n",
    "# NOTE: after exploration clean up the notebook \n",
    "#       only include relevant code, save exploration \n",
    "#       file, just incase you need it again \n",
    "\n",
    "# cell 5 \n",
    "# look for null values \n",
    "# df.isnull().sum() \n",
    "# df.isna().sum()\n",
    "# Precentage of missing values in each column \n",
    "# (df.isna().sum()/len(df))*100\n",
    "\n",
    "# cell 6 \n",
    "# check for duplicate rows \n",
    "# ID is usually the unique identifier \n",
    "# print(len(df['ID_Column']))\n",
    "# print(df['ID_Column'].nunique())\n",
    "# print(df.shape)\n",
    "# print(df['ID_Column'].unique().shape())\n",
    "\n",
    "# cell 7 \n",
    "# remove duplicates \n",
    "# df.drop_duplicates(subset = 'ID_Columns', keep = 'first', inplace = True)\n",
    "# check that duplicates have been removed \n",
    "# print(len(df['ID_Column'])\n",
    "# print(df['ID_Column'].nunique())\n",
    "\n",
    "# cell 8 \n",
    "# summary statistics \n",
    "# df.describe() \n",
    "# df.column.mean()\n",
    "# df.column.median()\n",
    "\n",
    "\n",
    "# cell 9 \n",
    "# using groupby to get the aggregated summary statistics \n",
    "# cust[['Occupation', 'YearlyIncome']].groupby('Occupation').median()\n",
    "\n",
    "\n",
    "# cell 10\n",
    "# correlation matrix \n",
    "# corr = df.corr()\n",
    "# print(corr)\n",
    "\n",
    "# Visualizations \n",
    "# Before creating a model data needs to be fully understood \n",
    "# The more time you spend exploring the data\n",
    "# - the less time you spend building bad models \n",
    "# - Whats redundant \n",
    "# - What's colinear\n",
    "# - What's noise \n",
    "# - Features are you most important aspect \n",
    "#   Feature engineering - categorical counts, discretiziation\n",
    "#                         log normal \n",
    "#                         exp, square, root \n",
    "#                         most transformtions are based on data that a is non-linear distribution \n",
    "#   Feature Engineering is an iterative process - must test\n",
    "#         lots of ideas and keep what works, fail fast and keep iterating\n",
    "# - Feature Engineering \n",
    "#   A. aggregating categories \n",
    "#   B. transforming a variable - log normal  \n",
    "#      \n",
    "#      np.log(df['col1'])\n",
    "#\n",
    "# - outliers could carry valuable information or could just throw the model off \n",
    "# - Understand the relationships in the data \n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# Histograms - looking for normality of data \n",
    "# Loop to look at lots of histograms \n",
    "# cols = ['HomeOwnerFlag', 'NumberCarsOwned',\n",
    "#         'NumberChildrenAtHome', 'TotalChildren',\n",
    "#         'YearlyIncome']\n",
    "\n",
    "# for i in cols:\n",
    "#     plt.hist(cust[i], bins = 10)\n",
    "#     plt.title(i)\n",
    "#     plt.show()\n",
    "#------------------------------------------\n",
    "# Boxplots - custom boxplot ordering\n",
    "# sns.boxplot(x = cust['Occupation'], y = cust['YearlyIncome'] )\n",
    "# order the boxplot based on category \n",
    "# sns.boxplot(x = cust['Occupation'], y = cust['YearlyIncome'], order = ['Management', 'Professional', 'Skilled Manual', \n",
    "#                                                                      'Clerical', 'Manual'])\n",
    "# Boxplot \n",
    "# 50% of the data is located in the inner box \n",
    "# 25% of the data is on either side of the box\n",
    "# the whiskers are at either 1.5*IQR(the box)\n",
    "# or at the lowest value on that side of the box\n",
    "# any value greater than 1.5*IQR is considered an outlier \n",
    "# \n",
    "# boxplot shows the median \n",
    "# in excel you can see the the mean represented with an x on the boxplot \n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------------------\n",
    "# KDE plot for when you have too many data points to\n",
    "# effectively visualize on an x - y plot \n",
    "# will show the densities for the points\n",
    "# too many points will result in points plotting on top \n",
    "# of each other and it is difficult to actually visualize \n",
    "# how many points are plotted at a particular point \n",
    "# Hex plotting is similar to KDE but may be better at \n",
    "# communicating some visual information that may have been lost \n",
    "# in a KDE plot (information lost if there is not a high enough\n",
    "# density of points, the hex plot will still register these outlier points \n",
    "# that may actually be of importance)\n",
    "\n",
    "# can also adjust transparency for overplotting \n",
    "# adjuct the alpha argument \n",
    "\n",
    "#----------------------------------------\n",
    "# sns.jointplot(x=, y=, data=)\n",
    "\n",
    "#----------------------------------------\n",
    "# sns.boxplot(x=, y=, data=, hue=)\n",
    "# hue for additional categorical information \n",
    "\n",
    "#----------------------------------------\n",
    "# sns.pairplot\n",
    "# create a correlation plot of pairs of variables to look for correlation \n",
    "# shows pair wise scatter plots \n",
    "\n",
    "#----------------------------------------\n",
    "# pairplot with custom kde plots on the upper diagonal \n",
    "# diagonal - straight line joining to different sides of a square or rectangle \n",
    "# num_cols = [\"curb_weight\", \"engine_size\", \"horsepower\", \"city_mpg\", \"price\", \"fuel_type\"] \n",
    "# sns.pairplot(auto_prices[num_cols], hue='fuel_type', palette=\"Set2\", diag_kind=\"kde\", height=2).map_upper(sns.kdeplot, cmap=\"Blues_d\")\n",
    "\n",
    "# pair plot without custom diagonal \n",
    "# cols = ['col1', 'col2', 'col3']\n",
    "# sns.pairplot(df[cols], size=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning \n",
    "\n",
    "# visualizing data for regression MPP example \n",
    "# - SAMPLE code using .replace\n",
    "## fix column names so the '-' character becomes '_'\n",
    "## cols = auto_prices.columns\n",
    "## auto_prices.columns = [str.replace('-', '_') for str in cols]\n",
    "\n",
    "# .loc - label based indexing \n",
    "# replacing values based on a label \n",
    "# auto_prices.loc[auto_prices[column] == '?', column] = np.nan\n",
    "\n",
    "# removing rows with na values \n",
    "# auto_prices.dropna(axis = 0, inplace = True)\n",
    "\n",
    "# convert columns to numeric types \n",
    "# df['column'] = pd.to_numeric(df['column'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of values \n",
    "# Prevents one feature from dominiating the model \n",
    "# Some features have large distributions of values \n",
    "# Larger distribution of valuse could dominate the model \n",
    "# even if they contain less predictive information that \n",
    "# other features \n",
    "\n",
    "# 2. options for scaling \n",
    "# A. z-score scaling - mean = 0 , standard deviation = 1 \n",
    "# B. min-max - all values between a scale of 0 - 1 \n",
    "#              lowest value is 0 highest value is 1 \n",
    "\n",
    "# Scikitlearn \n",
    "# from sklearn.preprocessing import scale \n",
    "# my_scaled_column = scale(df['col1'])\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Example code of creating scaled columns \n",
    "\n",
    "# from sklearn.preprocessing import scale\n",
    "# import pandas as pd\n",
    "# num_cols = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width']\n",
    "# iris_scaled = scale(iris[num_cols])\n",
    "# iris_scaled = pd.DataFrame(iris_scaled, columns = num_cols)\n",
    "# print(iris_scaled.describe().round(3))\n",
    "#--------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaing Data For Classification \n",
    "\n",
    "# Barplot \n",
    "# look at 0 and 1 as your categorical values \n",
    "# visualization of numeric values y-axis ,  classification values x-axis\n",
    "# look for separation to see if there is some separation \n",
    "\n",
    "# Histograms \n",
    "# compare histograms of counts \n",
    "# histogram of the 0 class\n",
    "# histogram of the 1 class \n",
    "# compare these histograms too see if it looks like there is separation\n",
    "\n",
    "# sns.distplot \n",
    "# combination of a histogram and and a kde plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to treat missing values \n",
    "# 1. Code for missing \n",
    "# 2. Remove record\n",
    "# 3. impute with mean \n",
    "# 4. impute with median \n",
    "# 5. regression based on other values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for specific character strings in a record \n",
    "# (df.astype(np.object) == '?').any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikitlearn\n",
    "# - requires numeric numpy arrays to function \n",
    "# - categorical values must be coded to numeric values  \n",
    "# - feature with high colinearity should not both be use in a machine learning model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial on pd.get_dummies \n",
    "\n",
    "# import pandas as pd \n",
    "# test = pd.Seriec(list('AABBCCABCDDEE'))\n",
    "\n",
    "# dummies_test = pd.get_dummies(test)\n",
    "\n",
    "# - creates a data frame with the dummy coding - 0 / 1 \n",
    "# convert out Series data do a data frame\n",
    "# test2 = pd.DataFrame(test)\n",
    "\n",
    "# Concatenate our data frames \n",
    "# result = pd.concat([test2, dummies_test], axis = 1, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas showing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
